{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09 - Classification of Roof Superstructures\n",
    "\n",
    "The notebook shows a multi-class classification with support vector machines on the application example of classifying roof superstructures from ALS data. For the application context, please refer to the respective slides in ISIS.\n",
    "\n",
    "To come straight to the point, the accuracies that can be achieved with the data are not particularly high. But one should not forget that the corresponding point subsets of the 3D point clouds could not be classified at all with conventional, empirical rule-bases approaches. Accordingly, the methodology is definitely an improvement.\n",
    "\n",
    "In addition to learning multi-class classification, which is not difficult at all from a programming point of view in scikit learn, the objective of this notebook is also the own critical interpretation of the results.\n",
    "\n",
    "The data records of the dataset stores geometric features computed for groups of points from an ALS 3D point cloud.\n",
    "\n",
    "**Description of Features:**\n",
    "1. **num_pts**: numper of points\n",
    "2. **max_height_diff**: maximum height difference between the highest and the lowest point in the segment\n",
    "3. **height_diff_to_build**: height difference between the highest point in building, which the segment belongs to, and the highest point in segment (Chimneys are typically the highest roof superstructure, and their value for this feature should then be 0.)\n",
    "4. **area_from_conv_hull**: area computed from convex hull of segment\n",
    "5. **area_from_alpha_shp**: area computed from alpha shape (The alpha-shape is something like a concave hull of the segment. Maybe check the figure in the wikipedia article: https://en.wikipedia.org/wiki/Alpha_shape)\n",
    "6. **min_elevation**: minimum elevation of the segment\n",
    "7. **max_elevation**: maximum elevation of the segment\n",
    "8. **avg_elevation**: average elevation of the segment\n",
    "9. **size_entropy**: (in short) the size of the segment in relation to the size of the building to which the segment belongs\n",
    "10. **elevation_entropy**: (in short) quantifies the degree of uncertainty provided by the heights of points constituting the segment\n",
    "11. **std_height**: standard deviation of the height values of the points\n",
    "12. **coef_variation**: coefficient variation of heights\n",
    "\n",
    "The **target classes** are 'shed dormer', 'gable dormer','ground', 'chimney', 'others' given as numeric values 1, 2, ..., 5. The class 'ground' seems out of place, since it is no roof superstructure. But some ground points in the dataset that are located close to a building were misclassified as building points (in the point cloud classification step) and they need now to be identified as ground. The class 'others' contains group of points that are superstructures, but which could not be identified by a human during the labeling process.\n",
    "\n",
    "Please note that those code parts that are mainly a repetition of previous notebooks are no longer greatly described and documented. Refer to the previous notebooks if something is unclear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/coursematerial/GIS/GeoDataScience/RoofSuperstructures.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "data_dir = str(Path.home()) + r'/coursematerial/GIS/GeoDataScience'\n",
    "\n",
    "filepath = os.path.join(data_dir, r'RoofSuperstructures.csv')\n",
    "\n",
    "print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "column_names = ['class', 'num_pts', 'max_height_diff', 'height_diff_to_build', 'area_from_conv_hull', 'area_from_alpha_shp', \n",
    "                'min_elevation', 'max_elevation', 'avg_elevation', 'size_entropy', 'elevation_entropy', 'std_height', 'coef_variation', 'building_id', 'segment_id']\n",
    "\n",
    "data_df = pd.read_csv(filepath, sep=' ', names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>num_pts</th>\n",
       "      <th>max_height_diff</th>\n",
       "      <th>height_diff_to_build</th>\n",
       "      <th>area_from_conv_hull</th>\n",
       "      <th>area_from_alpha_shp</th>\n",
       "      <th>min_elevation</th>\n",
       "      <th>max_elevation</th>\n",
       "      <th>avg_elevation</th>\n",
       "      <th>size_entropy</th>\n",
       "      <th>elevation_entropy</th>\n",
       "      <th>std_height</th>\n",
       "      <th>coef_variation</th>\n",
       "      <th>building_id</th>\n",
       "      <th>segment_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.27</td>\n",
       "      <td>7.84</td>\n",
       "      <td>0.389893</td>\n",
       "      <td>0.342041</td>\n",
       "      <td>288.55</td>\n",
       "      <td>288.82</td>\n",
       "      <td>288.692857</td>\n",
       "      <td>0.127143</td>\n",
       "      <td>2.514573</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.038571</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0.23</td>\n",
       "      <td>6.97</td>\n",
       "      <td>1.392822</td>\n",
       "      <td>0.815674</td>\n",
       "      <td>289.46</td>\n",
       "      <td>289.69</td>\n",
       "      <td>289.588462</td>\n",
       "      <td>0.101538</td>\n",
       "      <td>1.621488</td>\n",
       "      <td>0.128462</td>\n",
       "      <td>0.017692</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.84</td>\n",
       "      <td>0.081787</td>\n",
       "      <td>0.002197</td>\n",
       "      <td>293.84</td>\n",
       "      <td>294.12</td>\n",
       "      <td>293.981667</td>\n",
       "      <td>0.138333</td>\n",
       "      <td>0.584963</td>\n",
       "      <td>0.141667</td>\n",
       "      <td>0.046667</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.13</td>\n",
       "      <td>1.87</td>\n",
       "      <td>0.552734</td>\n",
       "      <td>0.097656</td>\n",
       "      <td>294.91</td>\n",
       "      <td>295.04</td>\n",
       "      <td>294.981429</td>\n",
       "      <td>0.058571</td>\n",
       "      <td>2.584963</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.018571</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.057129</td>\n",
       "      <td>0.066406</td>\n",
       "      <td>294.99</td>\n",
       "      <td>295.96</td>\n",
       "      <td>295.475000</td>\n",
       "      <td>0.485000</td>\n",
       "      <td>2.807355</td>\n",
       "      <td>0.485000</td>\n",
       "      <td>0.161667</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class  num_pts  max_height_diff  height_diff_to_build  area_from_conv_hull  \\\n",
       "0      1        7             0.27                  7.84             0.389893   \n",
       "1      3       13             0.23                  6.97             1.392822   \n",
       "2      5        6             0.28                  1.84             0.081787   \n",
       "3      3        7             0.13                  1.87             0.552734   \n",
       "4      5        6             0.97                  0.95             0.057129   \n",
       "\n",
       "   area_from_alpha_shp  min_elevation  max_elevation  avg_elevation  \\\n",
       "0             0.342041         288.55         288.82     288.692857   \n",
       "1             0.815674         289.46         289.69     289.588462   \n",
       "2             0.002197         293.84         294.12     293.981667   \n",
       "3             0.097656         294.91         295.04     294.981429   \n",
       "4             0.066406         294.99         295.96     295.475000   \n",
       "\n",
       "   size_entropy  elevation_entropy  std_height  coef_variation  building_id  \\\n",
       "0      0.127143           2.514573    0.142857        0.038571            0   \n",
       "1      0.101538           1.621488    0.128462        0.017692            0   \n",
       "2      0.138333           0.584963    0.141667        0.046667            1   \n",
       "3      0.058571           2.584963    0.071429        0.018571            2   \n",
       "4      0.485000           2.807355    0.485000        0.161667            2   \n",
       "\n",
       "   segment_id  \n",
       "0           8  \n",
       "1          10  \n",
       "2           4  \n",
       "3           5  \n",
       "4           6  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle & Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class **ShuffleSplit** allows to generate random permutations for given datasets that can be used for cross validation. (But be aware that the returned folds are not disjoint, meaning that data records may appear in several folds.) In the following, we provide the size of the test data (as a fraction), and some random state number to have repeatable results. The constructed **ShuffleSplit** object can then be used with the **split()** method on some data to get the indices for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generator"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "ss = ShuffleSplit(test_size=0.30, random_state=0)\n",
    "ss_gen = ss.split(data_df)\n",
    "type(ss_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **split()** method does not give the indices directly, but returns a generator object that can be used as an iterator to iterate through the folds, and retrieve the indices per fold. (By default, the number of splits is 10. But this can also be changed with the **n_splits** parameter of the **ShuffleSplit** constructor.\n",
    "\n",
    "(Because ss_gen is a generator, you can only iterate through it once. If you want to repeat the following for-loop, then you need to construct ss_gen once more.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  82 1327  438  998  349 1015  371  117 1337  252]   [ 317  587 1158  610 1241]\n",
      "[1061   27  674  647  559  509  424  628 1208  397]   [1363  812 1338 1045  446]\n",
      "[1045  161  908  347  712 1075  873 1102  780  412]   [ 848  733 1323  556 1085]\n",
      "[1097  113 1151  750  607  824  963  973  645  460]   [1169  806  878 1025 1143]\n",
      "[1039  816  852 1036 1318  722   19 1285  491 1162]   [  77  744  976 1159  844]\n",
      "[ 471  303  252  938  267 1127  341  921  574  996]   [ 807 1041 1380 1337  245]\n",
      "[ 349  105   71 1014   19   82  367  718  903 1180]   [118  90 334 640 919]\n",
      "[1205 1047  428  293  814  406 1263   48  638  900]   [163 246 310 612 884]\n",
      "[1049  863  872  430  811  631  892  635  530  353]   [ 375 1231  760  927  131]\n",
      "[ 432 1145  597 1014 1200 1354 1158  879  466  178]   [ 787 1101 1179  148  883]\n"
     ]
    }
   ],
   "source": [
    "for train_idx, test_idx in ss_gen:\n",
    "    print(train_idx[0:10], ' ', test_idx[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The for-loop uses the **next()** method of the generator object to get the data for the next iteration. (Every iterator and generator needs to provide this method, so that the object can be used in a for-loop.) If we just want one fold, then we can call the **next()** method directly on the result of the **split()** method. One fold of a training and testing split can be retrieved as a one-liner of code with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, test_idx = next(ShuffleSplit(test_size=0.30, random_state=0).split(data_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can extract the data records of training and testing using slicing on the data frame object.\n",
    "\n",
    "(Remember that we can use numeric indices starting with 0 when using **iloc** on rows and columns. Otherwise, we would need to work with the row indices and the column names of the data frame.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = data_df.iloc[train_idx, :]\n",
    "test_df = data_df.iloc[test_idx, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can extract the columns for input features and target labels for both training and testing data, again with slicing. (Of course, we could slice the rows and columns in one step.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_pts</th>\n",
       "      <th>max_height_diff</th>\n",
       "      <th>height_diff_to_build</th>\n",
       "      <th>area_from_conv_hull</th>\n",
       "      <th>area_from_alpha_shp</th>\n",
       "      <th>min_elevation</th>\n",
       "      <th>max_elevation</th>\n",
       "      <th>avg_elevation</th>\n",
       "      <th>size_entropy</th>\n",
       "      <th>elevation_entropy</th>\n",
       "      <th>std_height</th>\n",
       "      <th>coef_variation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>9</td>\n",
       "      <td>1.360</td>\n",
       "      <td>7.060</td>\n",
       "      <td>0.664795</td>\n",
       "      <td>0.555420</td>\n",
       "      <td>297.540</td>\n",
       "      <td>298.900</td>\n",
       "      <td>297.807778</td>\n",
       "      <td>1.092222</td>\n",
       "      <td>2.222392</td>\n",
       "      <td>0.267778</td>\n",
       "      <td>0.151111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1327</th>\n",
       "      <td>89</td>\n",
       "      <td>3.530</td>\n",
       "      <td>0.870</td>\n",
       "      <td>60.175537</td>\n",
       "      <td>2.346436</td>\n",
       "      <td>328.370</td>\n",
       "      <td>331.900</td>\n",
       "      <td>330.243146</td>\n",
       "      <td>1.656854</td>\n",
       "      <td>0.344446</td>\n",
       "      <td>1.873146</td>\n",
       "      <td>0.039663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>16</td>\n",
       "      <td>2.215</td>\n",
       "      <td>-1.885</td>\n",
       "      <td>2.134766</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>277.340</td>\n",
       "      <td>279.555</td>\n",
       "      <td>278.617813</td>\n",
       "      <td>0.937188</td>\n",
       "      <td>2.409391</td>\n",
       "      <td>1.277812</td>\n",
       "      <td>0.138438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>10</td>\n",
       "      <td>1.201</td>\n",
       "      <td>5.823</td>\n",
       "      <td>1.083496</td>\n",
       "      <td>0.176270</td>\n",
       "      <td>273.494</td>\n",
       "      <td>274.695</td>\n",
       "      <td>274.188200</td>\n",
       "      <td>0.506800</td>\n",
       "      <td>3.378512</td>\n",
       "      <td>0.694200</td>\n",
       "      <td>0.120100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>5</td>\n",
       "      <td>0.588</td>\n",
       "      <td>-1.190</td>\n",
       "      <td>0.196045</td>\n",
       "      <td>0.188965</td>\n",
       "      <td>277.102</td>\n",
       "      <td>277.690</td>\n",
       "      <td>277.482200</td>\n",
       "      <td>0.207800</td>\n",
       "      <td>3.104337</td>\n",
       "      <td>0.380200</td>\n",
       "      <td>0.117600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      num_pts  max_height_diff  height_diff_to_build  area_from_conv_hull  \\\n",
       "82          9            1.360                 7.060             0.664795   \n",
       "1327       89            3.530                 0.870            60.175537   \n",
       "438        16            2.215                -1.885             2.134766   \n",
       "998        10            1.201                 5.823             1.083496   \n",
       "349         5            0.588                -1.190             0.196045   \n",
       "\n",
       "      area_from_alpha_shp  min_elevation  max_elevation  avg_elevation  \\\n",
       "82               0.555420        297.540        298.900     297.807778   \n",
       "1327             2.346436        328.370        331.900     330.243146   \n",
       "438              0.968750        277.340        279.555     278.617813   \n",
       "998              0.176270        273.494        274.695     274.188200   \n",
       "349              0.188965        277.102        277.690     277.482200   \n",
       "\n",
       "      size_entropy  elevation_entropy  std_height  coef_variation  \n",
       "82        1.092222           2.222392    0.267778        0.151111  \n",
       "1327      1.656854           0.344446    1.873146        0.039663  \n",
       "438       0.937188           2.409391    1.277812        0.138438  \n",
       "998       0.506800           3.378512    0.694200        0.120100  \n",
       "349       0.207800           3.104337    0.380200        0.117600  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = train_df.iloc[:, 1:13]\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1327</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      class\n",
       "82        5\n",
       "1327      5\n",
       "438       1\n",
       "998       1\n",
       "349       4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = train_df.iloc[:, 0:1]\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_df.iloc[:, 1:13]\n",
    "y_test = test_df.iloc[:, 0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Class Balance\n",
    "\n",
    "Next, we take a look at the class balance of the data. First for the whole dataset, then for the training and testing. \n",
    "\n",
    "In all cases, we want a sorted output of the counted occurrences of the class labels (of y).\n",
    "\n",
    "For the whole dataset, we need to first slice the class column using iloc and numeric indices, convert the data frame into a NumPy array with the method **to_numpy()**, and get rid of the second dimension with **flatten()**. (When converting a data frame into a NumPy array, you end up with a 2D array.) The result (a 1D array) can be given to the **Counter** class. \n",
    "\n",
    "The **Counter** class will count the occurrences of every value it finds in the given data collection. And the method **items()** returns a dictionary of values found, and the number of occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 241)\n",
      "(3, 47)\n",
      "(5, 869)\n",
      "(4, 155)\n",
      "(2, 100)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(data_df.iloc[:, 0:1].to_numpy().flatten())\n",
    "for i in c.items():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a sorted output, we can use the **sorted()** function, which takes an iterable (any collection that can be used as an iterator (and which provides the **next()** method)) and makes a sorted list from the values the iterable provides. This list can then be used in a for-loop to print its content. To access the first and second value of the items returned by **items()**, we can use indexing with [0] and [1].\n",
    "\n",
    "A sorted output of classes according to their number of occurrences is then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : 241\n",
      "2 : 100\n",
      "3 : 47\n",
      "4 : 155\n",
      "5 : 869\n"
     ]
    }
   ],
   "source": [
    "for i in sorted(Counter(data_df.iloc[:, 0:1].to_numpy().flatten()).items()):\n",
    "    print(i[0], ':', i[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same for the training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : 171\n",
      "2 : 75\n",
      "3 : 38\n",
      "4 : 109\n",
      "5 : 595\n"
     ]
    }
   ],
   "source": [
    "for i in sorted(Counter(y_train.to_numpy().flatten()).items()):\n",
    "    print(i[0], ':', i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for the testing labels. (We could define a function for convenience, so that we do not need to type (or copy & paste) the same code over and over again.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : 70\n",
      "2 : 25\n",
      "3 : 9\n",
      "4 : 46\n",
      "5 : 274\n"
     ]
    }
   ],
   "source": [
    "for i in sorted(Counter(y_test.to_numpy().flatten()).items()):\n",
    "    print(i[0], ':', i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The split seems all right on first sight, and we get data records with the same class distribution in the whole dataset, the training subset, and the testing subset. However, class 5, which is the class for 'others', is highly overrepresented. This might pose a problem as these groups of points could not be identified by a human as a valid roof superstructure, but could end up having the same geometric properties as the other roof superstructures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine Model\n",
    "\n",
    "Next, we construct a SVM object and train it with the training data. Be careful that y_train is still a data frame and the **fit()** method of the support vector classifier wants a 1D array. So, we need to extract the class column.\n",
    "\n",
    "The hyperparameters (gamme and C) have been empirically determined. Once you got to the end of the notebook, you are asked to find better ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=32768, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma=1, kernel='rbf', max_iter=-1,\n",
       "    probability=False, random_state=None, shrinking=True, tol=0.001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel='rbf', gamma=1, C=32768)\n",
    "svc.fit(X_train, y_train['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the method **score()** of the **SVC** class, you can get the (mean) accuracy of the classifier giving the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6556603773584906"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.score(X_test, y_test['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets predict the classes from the test data using the **predict()** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svc.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which we can now use to get a classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " shed dormer       1.00      0.04      0.08        70\n",
      "gable dormer       1.00      0.08      0.15        25\n",
      "      ground       0.00      0.00      0.00         9\n",
      "     chimney       0.00      0.00      0.00        46\n",
      "      others       0.65      1.00      0.79       274\n",
      "\n",
      "    accuracy                           0.66       424\n",
      "   macro avg       0.53      0.22      0.20       424\n",
      "weighted avg       0.65      0.66      0.53       424\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "class_names = 'shed dormer', 'gable dormer','ground', 'chimney', 'others'\n",
    "print(classification_report(y_test['class'], y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the provided slides on ISIS that give some definitions of precision, recall, and f1-score.\n",
    "\n",
    "(Remember that all metrics are between 0 and 1, and that 0 (0%) means (simplified) bad, and 1 (100%) means good.)\n",
    "\n",
    "The **precision** gives us the proportion of the items that we correctly classified to belong to a certain class in comparison to the overall items that we identified to belong to this class. Both 'shed dormer' and 'gable dormer' have a high precision (1.0), which means that all data records that we identified as 'shed dormer' or 'gable dormer' are actually items of these classes. This seems to be a great result, because we actually wanted to identify roof superstructures. Although chimneys are not identified at all, but they also seem to not be many in the dataset anyway. And since we are not interested in ground and others, we can care less about those. **Is this a correct interpretation of the results?**\n",
    "\n",
    "To answer this question, we also have to take a look at **recall**. This metric gives us the proportion of the data records that belong to a class were actually identified correctly. So, it seems that we only identified 4% and 8% of the 'shed dormer' and 'gable dormer' objects in the dataset. So, our model missed almost all of the roof superstructures. The only class it got correct in this metric was others. All objects that are in reality 'others' are also classified as 'others'.\n",
    "\n",
    "The **f1-score** brings both precision and recall together, and we can see that only the class 'other's has a relevant value for the f1-score.\n",
    "\n",
    "And because the class 'others' is overrepresented, the overall (mean) accuracy is still high with 66%.\n",
    "\n",
    "To get more insight, lets us also take a look at the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shed dormer</th>\n",
       "      <th>gable dormer</th>\n",
       "      <th>ground</th>\n",
       "      <th>chimney</th>\n",
       "      <th>others</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>shed dormer</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gable dormer</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ground</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chimney</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>others</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              shed dormer  gable dormer  ground  chimney  others\n",
       "shed dormer             3             0       0        0      67\n",
       "gable dormer            0             2       0        0      23\n",
       "ground                  0             0       0        0       9\n",
       "chimney                 0             0       0        0      46\n",
       "others                  0             0       0        1     273"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "pd.DataFrame(cm, index=class_names, columns=class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take yourself some time and interpret the confusion matrix with what is given in the classification report and what is written above.\n",
    "\n",
    "What can be said as a final remark is that this classifier just classifies everything as 'others'. And because most of the data records are 'others', the accuracy is pretty high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Better SVM Pipeline\n",
    "\n",
    "As mentioned in the lecture video, SVMs are highly sensitive to unnormalized data. Therefore, we better normalize our features. This can be accomplished, e.g., with the **StandardScaler** class, which we apply to our data before feeding it to the support vector machine classifier class **SVC**. Because we need to apply the scaler to all our data that we feed to the **fit()** method, and also to the **predict()** method, we can build a pipeline that combines several classes together. Our pipeline is then the **StandardScaler** and the **SVC** class. All calls to **fit()** and **predict()** will go through all steps in the pipeline.\n",
    "\n",
    "The pipeline constructor takes a list of tuples, where the first element of the tuple gives the name of the pipeline step, and the second element of the tuple is the object to be called in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "svm_pipeline = Pipeline([\n",
    "('scaler', StandardScaler()),\n",
    "('svm_cls', SVC(kernel='rbf', gamma=0.2, C=32768000))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this pipeline in the same way as a stand-alone machine learning algorithm like SVC, e.g. to fit the model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('svm_cls',\n",
       "                 SVC(C=32768000, break_ties=False, cache_size=200,\n",
       "                     class_weight=None, coef0=0.0,\n",
       "                     decision_function_shape='ovr', degree=3, gamma=0.2,\n",
       "                     kernel='rbf', max_iter=-1, probability=False,\n",
       "                     random_state=None, shrinking=True, tol=0.001,\n",
       "                     verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_pipeline.fit(X_train, y_train['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the score (accuracy) for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.535377358490566"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_pipeline.score(X_test, y_test['class'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not as good as before. But better check the classification report from the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " shed dormer       0.32      0.47      0.38        70\n",
      "gable dormer       0.30      0.48      0.37        25\n",
      "      ground       0.16      0.33      0.21         9\n",
      "     chimney       0.24      0.20      0.21        46\n",
      "      others       0.76      0.62      0.68       274\n",
      "\n",
      "    accuracy                           0.54       424\n",
      "   macro avg       0.35      0.42      0.37       424\n",
      "weighted avg       0.59      0.54      0.55       424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_names = 'shed dormer', 'gable dormer','ground', 'chimney', 'others'\n",
    "print(classification_report(y_test['class'], y_pred, target_names=class_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the overall accuracy is a little lower, the precision, recall, and f1-score increases for (almost) all classes.\n",
    "\n",
    "Taking a look at the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shed dormer</th>\n",
       "      <th>gable dormer</th>\n",
       "      <th>ground</th>\n",
       "      <th>chimney</th>\n",
       "      <th>others</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>shed dormer</th>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gable dormer</th>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ground</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chimney</th>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>others</th>\n",
       "      <td>50</td>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              shed dormer  gable dormer  ground  chimney  others\n",
       "shed dormer            33             4       1        7      25\n",
       "gable dormer            5            12       0        1       7\n",
       "ground                  3             0       3        1       2\n",
       "chimney                13             2       3        9      19\n",
       "others                 50            22      12       20     170"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "pd.DataFrame(cm, index=class_names, columns=class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the confusion matrix actually has some values in the diagonal, which is very important. These are the correctly classified data records.\n",
    "\n",
    "The method is still rather weak with around 60%. One the one hand, 25 shed dormers were correctly identified. But on the other hand, more data records of the class 'others' were classified as shed dormers than there are actually shed dormers.\n",
    "\n",
    "Take a look on the classification report and confusion matrix and make sure you understand it.\n",
    "\n",
    "As final remarks:\n",
    "\n",
    "- The class 'others' might have a bad influence on the classifier, because these groups of points cannot be identified by a human. And they might as well be shed dormers or something else. Most likely, they are a mix of all kinds of superstructures, and maybe even some high vegetation. \n",
    "\n",
    "- If we would use this method to model further superstructures, we could very likely throw everything out that is predicted to belong to the predicted classes of 'others' and 'ground'. And we would end up with more superstructures on the 3D building models than before with around 40% of them being right.\n",
    "\n",
    "- We could also train the classifier to give output probabilities instead of class labels, and then use the method **predict_proba()** to get the probabilities of all classes instead of a class. And if the probability is low, we could ignore this prediction. Maybe this would give us a higher accuracy for the relevant classes.\n",
    "\n",
    "- We could address the imbalance of the classes. However, our experiments did not really work out well with this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Task:\n",
    "\n",
    "Try out different values for the hyperparameters gamma and C and observe how the overall accuracy (score), the classification report, and the confusion matrix develops. **It is important to develop a good intuition on how to interpret the quality of the model.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
